---
layout: post
title: Generalized Method of Moments (GMM) in R (Part 3 of 3)
subtitle: Explanation of 2SLS as GMM
gh-repo: AlfredSAM/alfredsam.github.io
gh-badge: [star, fork, follow]
cover-img: /assets/img/gmm.jpg
tags: [Econometrics, R, 2SLS, Generalized Methods of Moment, Instrumental Variables]
comments: true
---

This is the last post of the series about GMM, short for Generalized Method of Moments in Econometrics and Statistics. Interested audience are welcomed to review my fist post [Generalized Method of Moments (GMM) in R (Part 1 of 3)](https://alfredsam.github.io/2021-06-27-gmm-1/) for the basic ideas of GMM and the application using R package `gmm` and the second post [Generalized Method of Moments (GMM) in R (Part 2 of 3)](https://alfredsam.github.io/2021-07-04-gmm-2/) for the illustration about QMLE. From these two posts one can find:
* OLS and MLE estimators can be viewed as special cases of GMM estimators, and furthermore the MM estimators since the number of parameters is equal to the number of moment conditions. However, even OLS and MLE estimators are **exactly** equal to the MM estimators, one still needs to be careful with standard errors returned in the packages/softwares. Given the specific estimator, one can derive **different** sampling distributions (variances) based on distinct assumptions about _data generation process_. In general, such assumptions about _data generation process_ are subtle and hidden, and the users are probably to neglect them when applying the packages/softwarees. For example, OLS regression usually computes standard errors based on _homoskedasticity_ while MLE estimation always computes standard errors based on the **validity** of the distributional assumptions _by default_ in most of the packages/softwares. That's why we need to make sure that such underlyding assumptions are reliable in empirical studies. Of coz, the common strategy is to always indicate the **robustness** of the results, so the results with _robust_ standard errors are usually presented in papers.
* Without the knowledge of GMM (and the usage of `gmm` package), one can still obtain OLS and MLE estimators and the robust standard errors using specific packages, such as [`sandwich`](https://cran.r-project.org/web/packages/sandwich/sandwich.pdf) in R. Well, this is the case indeed to some degree. With the discussion of GMM, one can have a broader picture about the relations among estimators, but one can still just report the results of OLS and MLE by ignoring GMM. That is also the reason why GMM needs NOT be discussed in the first course Econometrics or probably any course especially for those PhD students who just wanna always rely on OLS and maybe some MLE. I believe brief discussion about **causal inference** may help to explain this issue. In this post, **causal inference** as the basic task of Econometrics and probably other scientific fields is briefly discussed at first. Secondly, **instrumental variables (IV)** and **Two Stage Least Square (2SLS)** are also discussed in the viewpoint of GMM.

## Causal Inference in Empirical Study

Across disciplines of social science (e.g. Economics, Psychology, Marketing, Management, etc.), various theories are developed to study the **causal inference** of some variable $x$ on the target variable $y$. In my first textbook _Economics_ (Samuelson, P., & Nordhaus, W., 2009), they illustrate the basic logic of Economics in the first Chapter and raise

{: .box-note}
**Note:** _"Remember to hold other things constant when you are analyzing the impact of a variable on the economic system."_

In general, when constructing some theories only relevant variables and factors are discussed and included in the system whether such system is expressed as mathematical form or not. It does not mean the target variable $$y$$ **ONLY** depends on the other variables within such system, but $$y$$ may also depends on the variables within other theoretical frameworks. Therefore, one should be careful that even though the **hypothesis** derived from some theoretical framework only involves $$y$$ and some other $$x$$ other relevant vatiables **NOT** shown up in such hypothesis should be treated as **constant**. For example, in _the law of demand_ saying that _the demand for some [**normal** good](https://en.wikipedia.org/wiki/Normal_good#:~:text=In%20economics%2C%20a%20normal%20good,referred%20as%20a%20normal%20good.) increases if its price decreases_, the complete picture about the factors influencing _the demand for some normal good_ involves also the individual's income, the prices of other goods, etc.. However, they are **NOT** shown in the expression of _the law of demand_, but all should have consensus that they are held **constant** for the specified effect.

In empirical study to quantify or test some **causal inference**, all of the relevant factors or variables, in addition to the $$x$$ shown in the **hypothesis**, to affect $$y$$ should be considered theoretically. Here is the gentle instruction about the _operationalized_ concept of **causal inference** in empirical study from Hansen's textbook (Hansen, Bruce E., 2021). Suppose the following function to decribe how $$y$$ depends on its all determinants:

$$y=h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right)$$

where $$x_1$$ is the _observed_ focal variable to be studied for the **causal inference**, $$\boldsymbol{x}_2$$ are _observed_ other factors to affect $$y$$, and $$\boldsymbol{u}$$ are other _unobserved_ factors to affect $$y$$. Please note that  $$\boldsymbol{u}$$ are _unobserved_ mainly due to (1) data availability or (2) the limitation of current theories. Therefore, such expression is quite general to describe **all** determinants to $$y$$. The **average causal effect (ACE)**  of $$x_1$$ on $$y$$ conditional on $$\boldsymbol{x}_2$$ is 

$$
\begin{aligned}
\operatorname{ACE}\left(x_{1}, \boldsymbol{x}_{2}\right) &=\mathbb{E}\left[C\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) \mid x_{1}, \boldsymbol{x}_{2}\right] \\
&=\int_{\mathbb{R}^{\ell}} \nabla_{1} h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right) d \boldsymbol{u}
\end{aligned}
$$ 

where $$f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right)$$ is the conditional density of $$\boldsymbol{u}$$ given $$x_1$$ and $$\boldsymbol{x}_2$$. Since $$\boldsymbol{u}$$ always exist and _unobserved_, the usual treatment is to integrate out them for the conditional expectation. That is, **average causal effect (ACE)** is the _operationalized_ concept in empirical study. Another highly relevant concept called **Conditional Expectation Function (CEF)** is also introduced:

$$
\begin{aligned}
m\left(x_{1}, \boldsymbol{x}_{2}\right) &=\mathbb{E}\left[h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) \mid x_{1}, \boldsymbol{x}_{2}\right] \\
&=\int_{\mathbb{R}^{\ell}} h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right) d \boldsymbol{u}
\end{aligned}
$$

Applying the marginal effect operator, the derivative of **CEF** with respect to $$x_1$$ is

$$
\begin{aligned}
\nabla_{1} m\left(x_{1}, \boldsymbol{x}_{2}\right) &=\int_{\mathbb{R}^{\ell}} \nabla_{1} h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right) d \boldsymbol{u} \\
&+\int_{\mathbb{R}^{\ell}} h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) \nabla_{1} f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right) d \boldsymbol{u} \\
&=\operatorname{ACE}\left(x_{1}, \boldsymbol{x}_{2}\right)+\int_{\mathbb{R}^{\ell}} h\left(x_{1}, \boldsymbol{x}_{2}, \boldsymbol{u}\right) \nabla_{1} f\left(\boldsymbol{u} \mid x_{1}, \boldsymbol{x}_{2}\right) d \boldsymbol{u}
\end{aligned}
$$

It is interesting that the derivative of **CEF**, which is popular and discussed prevantly in Econometrics and Statistics, is **NOT** equal to **ACE** in general. However, if conditional on $$\boldsymbol{x}_2$$, the random variables $$x_1$$ and $$\boldsymbol{u}$$ are **statistically independent** (**Conditional Independence Assumption (CIA)**) then 

$$\nabla_{1} m\left(x_{1}, x_{2}\right)=\operatorname{ACE}\left(x_{1}, x_{2}\right)$$

the **CEF** derivative equals the **average causal effect (ACE)**  for $$x_1$$ on $$y$$ conditional on $$\boldsymbol{x}_2$$. If **CIA** can be justified, then the following derivation is quite satifactory and somehow amazing. We can define **CEF error** $$e$$ as:

$$e=y-m(\boldsymbol{x})$$

By construction, this yields the formula:

$$y = m(\boldsymbol{x}) + e$$

The key property of the **CEF error** is that _it has a conditional mean of zero_, called **conditional mean independence**. This property is easily verified:

$$
\begin{aligned}
E[e \mid \boldsymbol{x}] &=\mathbb{E}[(y-m(\boldsymbol{x})) \mid \boldsymbol{x}] \\
&=\mathbb{E}[y \mid \boldsymbol{x}]-\mathbb{E}[m(\boldsymbol{x}) \mid \boldsymbol{x}] \\
&=m(\boldsymbol{x})-m(\boldsymbol{x}) \\
&=0
\end{aligned}
$$

This fact can be combined with the law of iterated expectations to show that

$$E[e \mid \boldsymbol{x}]=0$ (**conditional mean independence**) $\implies$ $\mathbb{E}[\boldsymbol{x} e]=\mathbf{0}$$

That is, if $$E[e \mid \boldsymbol{x}]$$ is assumed to be **linear** furthermore, then $$\boldsymbol{\beta}$$ in  $$m(\boldsymbol{x})=x_{1} \beta_{1}+x_{2} \beta_{2}+\cdots+\beta_{k}=\boldsymbol{x}^{\prime} \boldsymbol{\beta}$$ are the **projection coefficients** in **linear projection model**. That is perfect news, since **OLS** estimators are the consistent **estimators** for such **ACE**.

One point should be noticed that it is neither unrealistic nor unscientific for the researchers to solely rely on **OLS** regression or some **MLE** for their empirical studies to quantify and test the **ACE**. However, two conditions should be clarified. From the above discussion, **CIA** is the key to justify the equality between **CEF** derivative and **ACE**. If specific _control_ methods are implemented to guarantee **CIA**, then the researchers can always obtain such equality. The usual _control_ methods are referring to _experimental control methods_, especially [_random assignment_](https://en.wikipedia.org/wiki/Random_assignment). Nowadays, studies in Marketing and (Behavioral) Economics usually borrow the experimental methodology usually employed in Psychology, such that **CIA** and thus the independence between focal $$x_1$$ and other unobserved variables are supported. With _random assignment_ they just break down the correlation between $$x_1$$ and other _unobserved_ and rule out the alternative explanations, such that the change of expected $$y$$ can be safely attributed to the manipulation of $$x_1$$. Secondly, **linearity** for **CEF** is another key assumption. Well, consistent with the experimental designs only **several qualitative levels** of focal $$x_1$$ are discussed. Therefore, the exact functional form about the **qualitative** information seems not that important. Based on such situation, the training for the Ph.D students in several business fields relying on the _experimental studies_ usually covers the basics of Econometrics and Statistics but includes heavy loads on the _experimental designs_. On the other hand, the **hypothesis** concerned in those fields usually involves the **psychological constructs** (e.g. anxiety, happiness, etc.) which are **NOT** directly captured by _second-hand_ data. Therefore, the only way to conduct the empirical study on such **hypothesis** is to collect the _first-hand_ data in lab. The situation becomes complicated for the researchers relying on _second-hand_ data. In most of fields Economics, the **hypothesis** usually considers the observable variables only. Even though the underlying theories are built with some abstract terms, e.g. utility, the implied **hypothesis** should be observable. Nowadays, fields on Behavioral Economics, Mechanism Designs, and Game Theory also try employing lab experiments, but in most of the cases their focuses are still the observable but hardly quantified variables, e.g. some systems or mechanisms, not the psychological constructs directly. Therefore, most of the researchers in Economics need to deal with the _second-hand_ data which are high dimensional and usually correlated to each other. That is, **inequality** between **CEF** derivative and **ACE** is the common case, and the second or third courses of Econometrics are always needed.

## Instrumental Variables, 2 Stage Least Square, and GMM

Based on the theoretical frameworks, **structural model** or **structural equation** is the usual objective in empirical study, instead of **CEF**. The classical example is the study of the effect of _education_ on the _wage_:

$$\log (\text {wage})= \beta \text{education }+e$$

with $$\beta$$ the **average causal effect** of _education_ on _wage_. Before rushing to the samples and **OLS** regression, one can perceive that if _wage_ is affected by _unobserved ability_, and individuals with high ability self-select into higher education, then $$e$$ contains _unobserved ability_, so _education_ and $$e$$ will be _positively correlated_. Such simple discussion just destroys **CIA**. That is, $$\beta$$ here is **NOT** **projection coefficient**. We say that there is **endogeneity** in the linear model 

$$
y_{i}=\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}+e_{i} \label{eq1} \tag{1}
$$

if $$\boldsymbol{\beta}$$ is the _parameter of interest_ and $$\mathbb{E}\left[\boldsymbol{x}_{i} e_{i}\right] \neq \mathbf{0} \label{eq2} \tag{2}$$

Please note that **endogeneity** just precludes $$\eqref{eq1}$$ to be **linear CEF** or **linear projection model**. Given $$y_i$$ and $$\boldsymbol{x}_i$$ in $$\eqref{eq1}$$ we can nearly always cunstruct the **linear projection model**

$$\begin{aligned}
y_{i} &=\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}^{*}+e_{i}^{*} \\
\mathbb{E}\left[\boldsymbol{x}_{i} e_{i}^{*}\right] &=\boldsymbol{0}
\end{aligned}$$

However, under **endogeneity** $$\eqref{eq2}$$ the **projection coefficient** $$\boldsymbol{\beta}^{*}$$ does not equal the **structural parameter** $$\boldsymbol{\beta}$$:

$$\begin{aligned}
\boldsymbol{\beta}^{*} &=\left(\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\prime}\right]\right)^{-1} \mathbb{E}\left[\boldsymbol{x}_{i} y_{i}\right] \\
&=\left(\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\prime}\right]\right)^{-1} \mathbb{E}\left[\boldsymbol{x}_{i}\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}+e_{i}\right)\right] \\
&=\boldsymbol{\beta}+\left(\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\prime}\right]\right)^{-1} \mathbb{E}\left[\boldsymbol{x}_{i} e_{i}\right] \\
& \neq \boldsymbol{\beta}
\end{aligned}$$

the final relation since $$\mathbb{E}\left[\boldsymbol{x}_{i} e_{i}\right] \neq \mathbf{0}$$. Therefore, **OLS** estimator can **ONLY** be the **consistent** to the $$\boldsymbol{\beta}^{*}$$ but **NOT** $$\boldsymbol{\beta}$$.


























## References

* Chaussé, P. (2021). [Computing generalized method of moments and generalized empirical likelihood with R](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf). _Journal of Statistical Software_, 34(11), 1–35.

* Chausse, P., & Chausse, M. P. (2021). [Package ‘gmm’](https://cran.r-project.org/web/packages/gmm/gmm.pdf).

* Verbeek, M. (2004). [A Guide to Modern Econometrics (2nd edition)](https://thenigerianprofessionalaccountant.files.wordpress.com/2013/04/modern-econometrics.pdf). ERIM (Electronic) Books and Chapters. John Wiley&Sons, Chichester.

* Hansen, Bruce E. (2021). [Econometrics](https://www.ssc.wisc.edu/~bhansen/econometrics/Econometrics.pdf). Typescript, University of Wisconsin. Princeton University Press, forthcoming.

* Hansen, Bruce E. (2021). [Probability and Statistics for Economists](https://www.ssc.wisc.edu/~bhansen/probability/Probability.pdf). Typescript, University of Wisconsin. Princeton University Press, forthcoming.

* Samuelson, P., & Nordhaus, W. (2009). [EBOOK: Economics (19th)](http://pombo.free.fr/samunord19.pdf). McGraw Hill.

* White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica: journal of the Econometric Society, 817-838.



